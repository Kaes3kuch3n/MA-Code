{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Vocals Transcription\n",
    "This notebook contains the full pipeline for vocals transcription. The pipeline is composed of the following steps:\n",
    "1. Vocals Volume Normalization\n",
    "2. Effects Pipeline\n",
    "   1. Noise Gate\n",
    "   2. Lowpass Filter\n",
    "   3. Compressor\n",
    "3. Machine Learning Model (either MetricGAN or MTL or None)\n",
    "4. Note Prediction\n",
    "\n",
    "The following cell contains all the parameters that can be tuned for each step of the pipeline."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29d6e1b4b4081314"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "# Noise Gate\n",
    "noise_gate_threshold: float = -18.0\n",
    "noise_gate_attack: float = 500.0\n",
    "noise_gate_release: float = 1500.0\n",
    "\n",
    "# Lowpass Filter\n",
    "lowpass_cutoff: float = 500.0\n",
    "\n",
    "# Compressor\n",
    "compressor_threshold: float = -6.0\n",
    "compressor_ratio: float = 5.0\n",
    "compressor_attack: float = 1.0\n",
    "compressor_release: float = 100.0\n",
    "\n",
    "# Machine Learning Model\n",
    "ml_model: Union[str, None] = None\n",
    "\n",
    "# Basic Pitch\n",
    "basic_pitch_onset_threshold: float = 0.5\n",
    "basic_pitch_frame_threshold: float = 0.3\n",
    "basic_pitch_minimum_note_length: float = 127.7"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70cba0df41f871b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Execute this cell to use the resulting parameters from optimization run #2\n",
    "\n",
    "# Noise Gate\n",
    "noise_gate_threshold: float = -32.0\n",
    "noise_gate_attack: float = 70.0\n",
    "noise_gate_release: float = 2000.0\n",
    "\n",
    "# Lowpass Filter\n",
    "lowpass_cutoff: float = 1500.0\n",
    "\n",
    "# Compressor\n",
    "compressor_threshold: float = -18.0\n",
    "compressor_ratio: float = 2.0\n",
    "compressor_attack: float = 1.0\n",
    "compressor_release: float = 80.0\n",
    "\n",
    "# Machine Learning Model\n",
    "ml_model: Union[str, None] = None\n",
    "\n",
    "# Basic Pitch\n",
    "basic_pitch_onset_threshold: float = 0.2\n",
    "basic_pitch_frame_threshold: float = 0.43455654528869303\n",
    "basic_pitch_minimum_note_length: float = 80"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "134dd1b28713f1f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vocals Volume Normalization\n",
    "Uses pydub's normalize effect to bring the vocals volume to a standard level."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a648c05674b4104f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pydub import AudioSegment, effects\n",
    "\n",
    "\n",
    "def normalize_audio(in_path: str, out_path: str):\n",
    "    audio = AudioSegment.from_file(in_path)\n",
    "    normalized = effects.normalize(audio)\n",
    "    normalized.export(out_path, format=\"wav\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4de5488dd6f5a99f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Effects Pipeline\n",
    "Uses Spotify's pedalboard library to apply a series of effects to the vocals:\n",
    "1. Noise Gate\n",
    "2. Lowpass Filter\n",
    "3. Compressor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2771d764cc5efe52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pedalboard import Pedalboard, NoiseGate, Compressor, LowpassFilter\n",
    "from pedalboard.io import AudioFile\n",
    "\n",
    "# Setup pedalboard\n",
    "board = Pedalboard([\n",
    "    NoiseGate(threshold_db=noise_gate_threshold, attack_ms=noise_gate_attack, release_ms=noise_gate_release),\n",
    "    LowpassFilter(cutoff_frequency_hz=lowpass_cutoff),\n",
    "    Compressor(\n",
    "        threshold_db=compressor_threshold,\n",
    "        ratio=compressor_ratio,\n",
    "        attack_ms=compressor_attack,\n",
    "        release_ms=compressor_release,\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "def apply_pedalboard(in_path, out_path):\n",
    "    with AudioFile(in_path) as audio:\n",
    "        with AudioFile(out_path, 'w', audio.samplerate, audio.num_channels) as output:\n",
    "            # Loop over file and apply pedalboard effects\n",
    "            while audio.tell() < audio.frames:\n",
    "                chunk = audio.read(int(audio.samplerate))\n",
    "                effected = board.process(chunk, audio.samplerate, reset=False)\n",
    "                output.write(effected)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5433164609432f7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Machine Learning Model\n",
    "Uses SpeechBrain's pretrained MetricGAN or MTL models to enhance the vocals.\n",
    "The models are primarily used for speech enhancement, but let's check if they also work for music enhancement."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6c1e1c3a0b5cb27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from speechbrain.pretrained import SpectralMaskEnhancement, WaveformEnhancement\n",
    "\n",
    "metricgan_model = SpectralMaskEnhancement.from_hparams(\n",
    "    source=\"speechbrain/metricgan-plus-voicebank\",\n",
    "    savedir=\"../models/metricgan-plus-voicebank\",\n",
    ")\n",
    "mtl_model = WaveformEnhancement.from_hparams(\n",
    "    source=\"speechbrain/mtl-mimic-voicebank\",\n",
    "    savedir=\"../models/mtl-mimic-voicebank\",\n",
    ")\n",
    "\n",
    "\n",
    "def apply_metricgan(in_path, out_path):\n",
    "    noisy = metricgan_model.load_audio(in_path).unsqueeze(0)\n",
    "    enhanced = metricgan_model.enhance_batch(noisy, lengths=torch.tensor([1.]))\n",
    "    torchaudio.save(out_path, enhanced.cpu(), 16000)\n",
    "\n",
    "\n",
    "def apply_mtl(in_path, out_path):\n",
    "    enhanced = mtl_model.enhance_file(in_path)\n",
    "    torchaudio.save(out_path, enhanced.unsqueeze(0).cpu(), 16000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6efb2ae44912386"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Audio Optimization\n",
    "Applies the previously defined effects pipeline and the machine learning model to the vocals.\n",
    "The whole optimization pipeline in one handy function :)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a25091bf9f10a13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def optimize_audio(in_path: str, out_path: str, workdir: str):\n",
    "    if ml_model:\n",
    "        tmp_path = os.path.join(workdir, \"vocals_tmp.wav\")\n",
    "        apply_pedalboard(in_path, tmp_path)\n",
    "        if ml_model == \"metricgan\":\n",
    "            apply_metricgan(tmp_path, out_path)\n",
    "        elif ml_model == \"mtl\":\n",
    "            apply_mtl(tmp_path, out_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown ML model: {ml_model}\")\n",
    "    else:\n",
    "        apply_pedalboard(in_path, out_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19e42515a8317931"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Note Prediction\n",
    "Uses Spotify's Basic Pitch model to predict the notes from the optimized vocals track.\n",
    "Returns a PrettyMIDI object containing the predicted notes as instrument 0."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acae105e86c40904"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from basic_pitch import ICASSP_2022_MODEL_PATH\n",
    "from basic_pitch.inference import predict\n",
    "import tensorflow as tf\n",
    "\n",
    "basic_pitch_model = tf.saved_model.load(str(ICASSP_2022_MODEL_PATH))\n",
    "\n",
    "\n",
    "def predict_notes(in_path: str):\n",
    "    model_output, midi_data, note_events = predict(\n",
    "        in_path,\n",
    "        basic_pitch_model,\n",
    "        minimum_frequency=80,\n",
    "        maximum_frequency=1000,\n",
    "        onset_threshold=basic_pitch_onset_threshold,\n",
    "        frame_threshold=basic_pitch_frame_threshold,\n",
    "        minimum_note_length=basic_pitch_minimum_note_length,\n",
    "    )\n",
    "    return midi_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c622faadf3a800"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vocals Transcription Execution Cell\n",
    "Calls all the functions defined in the preceding cells to execute the full pipeline.\n",
    "\n",
    "Parameters:\n",
    "- `workdir`: The directory where the intermediate files will be stored.\n",
    "- `vocals_path`: The path to the vocals track to be transcribed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4f1847b67cb1e96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workdir = '../data/tmp'\n",
    "vocals_path = '../data/test/vocals.wav'\n",
    "\n",
    "if not os.path.exists(workdir):\n",
    "    os.makedirs(workdir)\n",
    "\n",
    "norm_vocals = os.path.join(workdir, \"vocals_normalized.wav\")\n",
    "opt_vocals = os.path.join(workdir, \"vocals_optimized.wav\")\n",
    "\n",
    "normalize_audio(vocals_path, norm_vocals)\n",
    "optimize_audio(norm_vocals, opt_vocals, workdir)\n",
    "midi = predict_notes(opt_vocals)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "397d1a96f96bcefa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Store transcription results\n",
    "from scipy.io import wavfile\n",
    "\n",
    "midi.write(os.path.join(workdir, \"vocals_transcribed.mid\"))\n",
    "audio = midi.synthesize()\n",
    "wavfile.write(os.path.join(workdir, \"vocals_transcribed.wav\"), 44100, audio)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "565a6f6a99f1743e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize transcription\n",
    "import note_seq\n",
    "import bokeh.io as io\n",
    "\n",
    "seq = note_seq.midi_file_to_note_sequence(os.path.join(workdir, \"test.mid\"))\n",
    "seq = note_seq.extract_subsequence(seq, 40, 46)\n",
    "plot1 = note_seq.plot_sequence(seq, False)\n",
    "io.export_svg(plot1, filename=os.path.join(workdir, \"vocals_transcribed.svg\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bb8e2e7a1d74078"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
